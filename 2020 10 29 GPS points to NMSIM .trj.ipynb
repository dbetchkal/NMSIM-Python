{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DENA aviation GPS points $\\rightarrow$ NMSIM trajectories\n",
    "\n",
    "Davyd_Betchkal@nps.gov â—˜ 2020-10-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some very standard libraries\n",
    "import sys\n",
    "import datetime as dt\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "# geoprocessing libraries\n",
    "import fiona\n",
    "from fiona.crs import from_epsg\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import mapping, Point, Polygon\n",
    "\n",
    "# load the DENA query tracks script\n",
    "sys.path.append(r\"D:\\overflights\\scripts\")\n",
    "from query_tracks import query_tracks\n",
    "\n",
    "# load iyore\n",
    "sys.path.append(r\"C:\\Users\\ahug\\Documents\\PythonScripts\\iyore\")\n",
    "import iyore\n",
    "\n",
    "\n",
    "# ===========================  define functions  =======================================\n",
    "\n",
    "def get_utm_zone(longitude):\n",
    "    \n",
    "    return (int(1+(longitude+180.0)/6.0))\n",
    "\n",
    "def climb_angle(v):\n",
    "    \n",
    "    \"\"\"\n",
    "    compute the 'climb angle' of a vector\n",
    "    A = ð‘›â€¢ð‘=|ð‘›||ð‘|ð‘ ð‘–ð‘›(ðœƒ)\n",
    "    \"\"\"\n",
    "    \n",
    "    # a unit normal vector perpendicular to the xy plane\n",
    "    n = np.array([0,0,1])\n",
    "    \n",
    "    degrees = np.degrees(np.arcsin( np.dot(n, v)/(np.linalg.norm(n)*np.linalg.norm(v))))\n",
    "    return degrees\n",
    "\n",
    "def point_buffer(lat, lon, km):\n",
    "    \n",
    "    wgs84 = pyproj.Proj(init='epsg:4326')\n",
    "\n",
    "    # Azimuthal equidistant projection\n",
    "    aeqd_formatter = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n",
    "    aeqd = pyproj.Proj(aeqd_formatter.format(lat=lat, lon=lon))\n",
    "\n",
    "    # project the site coordinate into aeqd\n",
    "    long_m, lat_m = pyproj.transform(wgs84, aeqd, lon, lat)\n",
    "\n",
    "    # buffer using a radius in meters\n",
    "    buf_m = Point(long_m, lat_m).buffer(km * 1000)  # distance in metres\n",
    "\n",
    "    # this will convert polygons from aeqd back into wgs84\n",
    "    projector = partial(pyproj.transform, aeqd, wgs84)\n",
    "\n",
    "    buf = transform(projector, buf_m)  # apply projection\n",
    "\n",
    "    return buf\n",
    "\n",
    "\n",
    "def tracks_within(ds, site, year, search_within_km = 20, climb_ang_max = 20, aircraft_specs=False, clip=False, altOut=None):\n",
    "    \n",
    "    unit = \"DENA\"\n",
    "    \n",
    "    # ===== first part; site coordinate wrangling =====================\n",
    "    \n",
    "    # load the metadata sheet\n",
    "    metadata = pd.read_csv(r\"V:\\Complete_Metadata_AKR_2001-2020.txt\", delimiter=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # look up the site's coordinates in WGS84\n",
    "    lat_in, long_in = metadata.loc[(metadata[\"code\"] == site)&(metadata[\"year\"] == year), \"lat\":\"long\"].values[0]\n",
    "\n",
    "    # lookup the UTM zone using the first point\n",
    "    zone = get_utm_zone(long_in)\n",
    "\n",
    "    # epsg codes for Alaskan UTM zones\n",
    "    epsg_lookup = {1:'epsg:26901', 2:'epsg:26902', 3:'epsg:26903', 4:'epsg:26904', 5:'epsg:26905', \n",
    "                   6:'epsg:26906', 7:'epsg:26907', 8:'epsg:26908', 9:'epsg:26909', 10:'epsg:26910'}\n",
    "\n",
    "    # convert from D.d (WGS84) to meters (NAD83)\n",
    "    in_proj = pyproj.Proj(init='epsg:4326')\n",
    "    out_proj = pyproj.Proj(init=epsg_lookup[zone])\n",
    "\n",
    "    # convert into NMSIM's coordinate system\n",
    "    long, lat = pyproj.transform(in_proj, out_proj, long_in, lat_in)\n",
    "\n",
    "\n",
    "    # ===== second part; mic height to feet =====================\n",
    "\n",
    "    # look up the site's coordinates in WGS84\n",
    "    height = metadata.loc[(metadata[\"code\"] == site)&(metadata[\"year\"] == year), \"microphone_height\"].values[0]\n",
    "\n",
    "    print(unit+site+str(year)+\":\", \"{0:.0f},\".format(long), \"{0:.0f}\".format(lat), \"- UTM zone\", zone)\n",
    "    print(\"\\tmicrophone height\", \"{0:.2f} feet.\".format(height*3.28084))\n",
    "\n",
    "\n",
    "    # ===== third part; save mask file using the buffer radius of choice ===============\n",
    "\n",
    "    # create the buffer polygon\n",
    "    buf = point_buffer(lat_in, long_in, search_within_km)  \n",
    "\n",
    "    # Define a polygon feature geometry with one attribute\n",
    "    schema = {'geometry': 'Polygon', 'properties': {'id': 'int'},}\n",
    "\n",
    "    # write a new shapefile with the buffered polygon\n",
    "    with fiona.open('site_buf.shp', 'w', 'ESRI Shapefile', schema, crs=from_epsg(4326)) as c:\n",
    "        \n",
    "        ## If there are multiple geometries, put the \"for\" loop here\n",
    "        c.write({'geometry': mapping(buf),'properties': {'id': 0},})\n",
    "\n",
    "    print(\"\\n\\tShapefile containing \" + str(search_within_km)+\"km radius buffer has been written!\")\n",
    "    \n",
    "    # plot the buffer within the park boundary\n",
    "    buffer_path = r\"C:\\Users\\ahug\\Documents\\PythonScripts\\site_buf.shp\"\n",
    "    DENA_outline_path = r\"T:\\ResMgmt\\WAGS\\Sound\\GIS\\Denali\\DENA_outline.shp\"\n",
    "    gpd_buffer = gpd.read_file(buffer_path)\n",
    "    gpd_DENA = gpd.read_file(DENA_outline_path)\n",
    "    base = gpd_buffer.plot(color='white', edgecolor='black')\n",
    "    base.set_aspect(2)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # ===== fourth part; determine the date range of NVSPL files ===============\n",
    "\n",
    "    # load the datetime of every NVSPL file\n",
    "    NVSPL_dts = pd.Series([dt.datetime(year=int(e.year),\n",
    "                                       month=int(e.month),\n",
    "                                       day=int(e.day),\n",
    "                                       hour=int(e.hour),\n",
    "                                       minute=0,\n",
    "                                       second=0) for e in ds.nvspl(unit=unit, site=site, year=year)])\n",
    "\n",
    "    # everything should be in chronological order, but just in case...\n",
    "    NVSPL_dts.sort_values(inplace=True, ascending=True)\n",
    "\n",
    "    # retrieve the start/end bounds and convert back to YYYY-MM-DD strings\n",
    "    start, end = (dt.datetime.strftime(d, \"%Y-%m-%d\") for d in [NVSPL_dts.iloc[0], NVSPL_dts.iloc[-1]])\n",
    "    print(\"\\n\\tRecord begins\", start, \"and ends\", end, \"\\n\")\n",
    "    \n",
    "    # load tracks from the database over a certain daterange, using the buffered site\n",
    "    tracks = query_tracks(connection_txt=r\"D:\\overflights\\config\\connection_info.txt\", \n",
    "                          start_date=start, end_date=end, \n",
    "                          mask=gpd_buffer, clip_output=clip,\n",
    "                          aircraft_info=aircraft_specs)\n",
    "    \n",
    "    # make a dataframe to hold distances and times\n",
    "    closest_approaches = pd.DataFrame([], index=np.unique(tracks[\"id\"]), columns=[\"closest_distance\", \"closest_time\"])\n",
    "    \n",
    "    # describe the tracks intersecting the polygon\n",
    "    for f_id, data in tracks.groupby(\"flight_id\"):\n",
    "        \n",
    "        if(len(data) > 1):\n",
    "        \n",
    "            # double check that the data are sorted by time\n",
    "            data = data.sort_values(\"ak_datetime\")\n",
    "\n",
    "            long_utm, lat_utm = pyproj.transform(in_proj, out_proj, data[\"longitude\"].values, data[\"latitude\"].values)\n",
    "\n",
    "            # assign back into the dataframe for subsequent use\n",
    "            data.loc[:, \"long_UTM\"] = long_utm\n",
    "            data.loc[:, \"lat_UTM\"] = lat_utm\n",
    "\n",
    "            # coordinates of each point\n",
    "            coords = np.array([[lo, la, e] for lo, la, e in zip(data[\"long_UTM\"], \n",
    "                                                                data[\"lat_UTM\"], \n",
    "                                                                0.3048*data[\"altitude_ft\"])])\n",
    "\n",
    "            # convert the coordinates to vectors\n",
    "            V = np.diff(coords, axis=0)\n",
    "\n",
    "            # compute the climb angle for each point: \n",
    "            climb_angs = np.array([climb_angle(V[n]) for n in np.arange(len(V))])\n",
    "\n",
    "            # if the climb angle is greater than 20Â° something strange is going on: reset to zero\n",
    "            climb_angs[np.abs(climb_angs) > climb_ang_max] = 0\n",
    "\n",
    "            # we don't know what the last point's angle should be, but NMSIM requires a value\n",
    "            # just repeat the penultimate point's value\n",
    "            climb_angs = np.append(climb_angs, climb_angs[-1])\n",
    "\n",
    "            # assign the climb angles to the array\n",
    "            data[\"ClimbAngle\"] = climb_angs\n",
    "\n",
    "            # NMSIM probably won't like nan... replace those with zero as well\n",
    "            data[\"ClimbAngle\"].fillna(0, inplace=True)\n",
    "\n",
    "            # this is the start time of the track\n",
    "            start = data[\"ak_datetime\"].iloc[0]\n",
    "\n",
    "            # truncate starting time to nearest hour to check against the NVSPL list\n",
    "            check = start.replace(minute=0, second=0)\n",
    "\n",
    "            # 1 if there is a match, 0 if not\n",
    "            match_bool = NVSPL_dts.apply(lambda date: date in [check]).sum()\n",
    "\n",
    "            if(match_bool == 0):\n",
    "\n",
    "                tracks = tracks[tracks[\"flight_id\"] != f_id]\n",
    "                print(\"\\t\\t\", \"Flight starting\", start, \"has no matching acoustic record\")\n",
    "\n",
    "            elif(match_bool == 1):\n",
    "                \n",
    "                if(altOut is not None):\n",
    "\n",
    "                    # make a folder for NMSIM .trj outputs\n",
    "                    trj_out = altOut\n",
    "                    if not os.path.exists(trj_out):\n",
    "                        os.makedirs(trj_out)\n",
    "                        \n",
    "                else:\n",
    "                    \n",
    "                    # a path to the site's computational outputs folder\n",
    "                    cOut = [e.path for e in ds.dataDir(unit=unit, site=site, year=year)][0] + \\\n",
    "                            os.sep + \"02 ANALYSIS\\Computational Outputs\"\n",
    "                    \n",
    "                    # make a folder for NMSIM .trj outputs\n",
    "                    trj_out = cOut + os.sep + \"NMSIM_trj\"\n",
    "                    if not os.path.exists(trj_out):\n",
    "                        os.makedirs(trj_out)\n",
    "\n",
    "                # find the time at which the flight passes closest to the station\n",
    "                site_coords = np.array([long, lat])\n",
    "                GPSpoints_xy = np.array([data[\"long_UTM\"], data[\"lat_UTM\"]])\n",
    "\n",
    "                # which point made the closest approach to the site?\n",
    "                min_distance = np.min(np.linalg.norm(site_coords - GPSpoints_xy.T, axis=1))/1000\n",
    "\n",
    "                # keep track of the closest approach by id\n",
    "                closest_approaches.loc[data[\"flight_id\"].iloc[0], \"closest_distance\"] = min_distance\n",
    "\n",
    "                # at what time did the closest approach occur?\n",
    "                closest_time = data.iloc[np.argmin(np.linalg.norm(site_coords - GPSpoints_xy.T, axis=1))]['ak_datetime']\n",
    "\n",
    "                # likewise keep track of the closest time\n",
    "                closest_approaches.loc[data[\"flight_id\"].iloc[0], \"closest_time\"] = closest_time\n",
    "\n",
    "                print(\"\\t\\t\", \"#\"+str(f_id), \"expected closest at\", closest_time, \"{0:0.1f}km\".format(min_distance))\n",
    "\n",
    "                # create a time-elapsed column\n",
    "                data[\"time_elapsed\"] = (data[\"ak_datetime\"] - data[\"ak_datetime\"].min()).apply(lambda t: t.total_seconds())\n",
    "\n",
    "\n",
    "                if(min_distance <= search_within_km):\n",
    "\n",
    "                    # ======= densify the GPS points for NMSIM ========\n",
    "\n",
    "                    print(\"\\n\\t\\t\\t\", \"Flight is within search distance. Attempting to densify\", \n",
    "                          data.shape[0], \"points...\")\n",
    "\n",
    "                    new_points = gpd.GeoDataFrame([])\n",
    "                    for row in data.itertuples():\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            next_ind = data.index[np.argwhere(data.index == row.Index)+1][0][0]\n",
    "\n",
    "                            # this represents time steps < 1 second\n",
    "                            interpSteps = int(1.1*(data.loc[next_ind, \"ak_datetime\"] - row.ak_datetime).total_seconds())\n",
    "                            print(\"trying for\", interpSteps, \"steps between points\")\n",
    "\n",
    "                            # interpolate the indices, longitudes, latitudes, and altitudes\n",
    "                            # we don't need the first or last values - they're already in the dataframe\n",
    "                            indi = np.linspace(row.Index, next_ind, interpSteps)[1:-1]\n",
    "                            ti = np.linspace(row.time_elapsed, data.loc[next_ind, \"time_elapsed\"], interpSteps)[1:-1]\n",
    "                            xi = np.linspace(row.longitude, data.loc[next_ind, \"longitude\"], interpSteps)[1:-1]\n",
    "                            yi = np.linspace(row.latitude, data.loc[next_ind, \"latitude\"], interpSteps)[1:-1]\n",
    "                            zi = np.linspace(row.altitude_ft, data.loc[next_ind, \"altitude_ft\"], interpSteps)[1:-1]\n",
    "                            utm_xi = np.linspace(row.long_UTM, data.loc[next_ind, \"long_UTM\"], interpSteps)[1:-1]\n",
    "                            utm_yi = np.linspace(row.lat_UTM, data.loc[next_ind, \"lat_UTM\"], interpSteps)[1:-1]\n",
    "                            cai = np.linspace(row.ClimbAngle, data.loc[next_ind, \"ClimbAngle\"], interpSteps)[1:-1]\n",
    "                            hi = np.linspace(row.heading, data.loc[next_ind, \"heading\"], interpSteps)[1:-1]\n",
    "                            vi = np.linspace(row.knots, data.loc[next_ind, \"knots\"], interpSteps)[1:-1]\n",
    "\n",
    "                            # generate geometry objects for each new interpolated point\n",
    "                            gi = [Point(xyz) for xyz in zip(xi, yi, zi)]\n",
    "\n",
    "                            # create a dictionary of the interpolated values to their column\n",
    "                            d = {'time_elapsed': ti,\n",
    "                                 'longitude': xi, \n",
    "                                 'latitude': yi,\n",
    "                                 'altitude_ft': zi,\n",
    "                                 'long_UTM': utm_xi,\n",
    "                                 'lat_UTM': utm_yi,\n",
    "                                 'ClimbAngle': cai,\n",
    "                                 'heading': hi,\n",
    "                                 'knots': vi,\n",
    "                                 'geom': gi}\n",
    "\n",
    "\n",
    "                            # turn the newly interpolated values into a GeoDataFrame \n",
    "                            rowsi = gpd.GeoDataFrame(d, index=indi, crs=\"EPSG:4326\")\n",
    "\n",
    "                            # append to the track's overall new points\n",
    "                            new_points = new_points.append(rowsi)\n",
    "\n",
    "                        # there is no next index on the last row... pass through\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "\n",
    "\n",
    "                    # append the new points and sort by index (which is also by time)\n",
    "                    data = data.append(new_points)\n",
    "                    data = data.sort_index()\n",
    "\n",
    "                    print(\"\\t\\t\\t\", \"...trajectory now has\", \n",
    "                          data.shape[0], \"points!\\n\")\n",
    "\n",
    "                    # ======= write the trajectory file! ==============\n",
    "\n",
    "                    print(\"\\t\\t\\t\", \"Densification complete, writing trajectory file...\")\n",
    "\n",
    "                    # add N-number and begin time\n",
    "                    start_time = dt.datetime.strftime(data[\"utc_datetime\"].min(), \"%Y-%m-%d %H:%M:%S\")\n",
    "                    file_name_dt = dt.datetime.strftime(data[\"utc_datetime\"].min(), \"_%Y%m%d_%H%M%S\")\n",
    "                    N_number = data[\"registration\"].iloc[0]\n",
    "\n",
    "                    # path to the specific .trj file to be written\n",
    "                    trj_path = trj_out + os.sep + str(N_number) + str(file_name_dt) + \".trj\"\n",
    "\n",
    "                    with open(trj_path, 'w') as trajectory:\n",
    "\n",
    "                        # write the header information\n",
    "                        trajectory.write(\"Flight track trajectory variable description:\\n\")\n",
    "                        trajectory.write(\" time - time in seconds from the reference time\\n\")\n",
    "                        trajectory.write(\" Xpos - x coordinate (UTM)\\n\")\n",
    "                        trajectory.write(\" Ypos - y coordinate (UTM)\\n\")\n",
    "                        trajectory.write(\" UTM Zone  \"+str(zone)+\"\\n\")\n",
    "                        trajectory.write(\" Zpos - z coordinate in meters MSL\\n\")\n",
    "                        trajectory.write(\" heading - aircraft compass bearing in degrees\\n\")\n",
    "                        trajectory.write(\" climbANG - aircraft climb angle in degrees\\n\")\n",
    "                        trajectory.write(\" vel - aircraft velocity in knots\\n\")\n",
    "                        trajectory.write(\" power - % engine power\\n\")\n",
    "                        trajectory.write(\" roll - bank angle (right wing down), degrees\\n\")\n",
    "                        trajectory.write(\"FLIGHT \" + str(N_number) + \" beginning \" + start_time +\" UTC\\n\")\n",
    "                        trajectory.write(\"TEMP.  59.0\\n\")\n",
    "                        trajectory.write(\"Humid.  70.0\\n\")\n",
    "                        trajectory.write(\"\\n\")\n",
    "                        trajectory.write(\"         time(s)        Xpos           Ypos           Zpos         heading        climbANG       Vel            power          rol\\n\")\n",
    "\n",
    "                        # now write the data section row by row\n",
    "                        for ind, point in data.iterrows():\n",
    "\n",
    "                            # write the line\n",
    "                            trajectory.write(\"{0:15.3f}\".format(point[\"time_elapsed\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"long_UTM\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"lat_UTM\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(0.3048*point[\"altitude_ft\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"heading\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"ClimbAngle\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(point[\"knots\"]) + \\\n",
    "                                             \"{0:15.3f}\".format(95) + \\\n",
    "                                             \"{0:15.3f}\".format(0) + \"\\n\")\n",
    "\n",
    "                        print(\"\\t\\t\\t...finished writing .trj\", \"\\n\")\n",
    "                        print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    if(tracks.size == 0):\n",
    "        \n",
    "        print(\"\\nSorry, no tracks in the database coincide with this deployment.\")\n",
    "    \n",
    "    elif(tracks.size > 0):\n",
    "        \n",
    "        u = np.unique(tracks[\"id\"])\n",
    "        print(\"\\nThere are\", len(u), \"tracks in the database which coincide with this deployment.\")\n",
    "        print(\"Identification numbers:\", u)\n",
    "        \n",
    "        # iterate through each id number and add the closest approach information\n",
    "        for track_id, flight in closest_approaches.iterrows():\n",
    "            \n",
    "            tracks.loc[tracks[\"flight_id\"] == track_id, \"closest_time\"] = flight[\"closest_time\"]\n",
    "            tracks.loc[tracks[\"flight_id\"] == track_id, \"closest_distance\"] = flight[\"closest_distance\"]\n",
    "        \n",
    "        return tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Reference:</font> using `iyore` what sites correspond to the GPS data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = iyore.Dataset(r\"E:\\Sound Data\") # define a dataset object\n",
    "\n",
    "# find Denali sites from 2019, 2020\n",
    "np.unique([e.site for e in archive.dataDir(unit=\"DENA\", year=[\"2019\", \"2020\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GPS points to NMSIM *.trj*\n",
    "Given a NPS monitoring site... and how far away are you interested in vehicle motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive = iyore.Dataset(r\"E:\\Sound Data\") # \\\\INPDENARENDER\n",
    "\n",
    "# ============= EDIT THESE ===================================================\n",
    "\n",
    "# project directory for the current site of interest\n",
    "project_dir = r\"C:\\Users\\ahug\\Documents\\NMSim_2014\\Data\\DENABULL\"\n",
    "\n",
    "year = 2019\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# extract the site name from the project directory path\n",
    "site = project_dir[-4:]\n",
    "\n",
    "tracks = tracks_within(archive, site, year, search_within_km = 20, aircraft_specs=False,\n",
    "                      altOut = project_dir + os.sep + r\"Input_Data\\03_TRAJECTORY\")\n",
    "\n",
    "\n",
    "# which N-Numbers were observed on this record?\n",
    "tracks.registration.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "obs_aircraft = {\"RUTH\":['N8888', 'N72309', 'N619CH', 'N72395', 'N74PS', 'N570AE'],\n",
    "                \"TRLA\":['N21HY', 'N709M', 'N473YC', 'N619CH', 'N72309', 'N74PS', 'N570AE'],\n",
    "                \"TEK4\":['N21HY', 'N8888', 'N72309', 'N619CH', 'N74PS']}\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal = tracks.loc[tracks.id == 3710, :]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n",
    "tracks.plot(column='altitude_ft', markersize=2, cmap=\"terrain\", ax=ax, legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
